
R version 2.5.0 beta (2007-04-10 r41119)
Copyright (C) 2007 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ### * <HEADER>
> ###
> attach(NULL, name = "CheckExEnv")
> assign("nameEx", 
+        local({
+ 	   s <- "__{must remake R-ex/*.R}__"
+            function(new) {
+                if(!missing(new)) s <<- new else s
+            }
+        }),
+        pos = "CheckExEnv")
> ## Add some hooks to label plot pages for base and grid graphics
> assign("base_plot_hook",
+        function() {
+            pp <- par(c("mfg","mfcol","oma","mar"))
+            if(all(pp$mfg[1:2] == c(1, pp$mfcol[2]))) {
+                outer <- (oma4 <- pp$oma[4]) > 0; mar4 <- pp$mar[4]
+                mtext(sprintf("help(\"%s\")", nameEx()), side = 4,
+                      line = if(outer)max(1, oma4 - 1) else min(1, mar4 - 1),
+               outer = outer, adj = 1, cex = .8, col = "orchid", las=3)
+            }
+        },
+        pos = "CheckExEnv")
> assign("grid_plot_hook",
+        function() {
+            pushViewport(viewport(width=unit(1, "npc") - unit(1, "lines"),
+                                  x=0, just="left"))
+            grid.text(sprintf("help(\"%s\")", nameEx()),
+                      x=unit(1, "npc") + unit(0.5, "lines"),
+                      y=unit(0.8, "npc"), rot=90,
+                      gp=gpar(col="orchid"))
+        },
+        pos = "CheckExEnv")
> setHook("plot.new",     get("base_plot_hook", pos = "CheckExEnv"))
> setHook("persp",        get("base_plot_hook", pos = "CheckExEnv"))
> setHook("grid.newpage", get("grid_plot_hook", pos = "CheckExEnv"))
> assign("cleanEx",
+        function(env = .GlobalEnv) {
+ 	   rm(list = ls(envir = env, all.names = TRUE), envir = env)
+            RNGkind("default", "default")
+ 	   set.seed(1)
+    	   options(warn = 1)
+ 	   .CheckExEnv <- as.environment("CheckExEnv")
+ 	   delayedAssign("T", stop("T used instead of TRUE"),
+ 		  assign.env = .CheckExEnv)
+ 	   delayedAssign("F", stop("F used instead of FALSE"),
+ 		  assign.env = .CheckExEnv)
+ 	   sch <- search()
+ 	   newitems <- sch[! sch %in% .oldSearch]
+ 	   for(item in rev(newitems))
+                eval(substitute(detach(item), list(item=item)))
+ 	   missitems <- .oldSearch[! .oldSearch %in% sch]
+ 	   if(length(missitems))
+ 	       warning("items ", paste(missitems, collapse=", "),
+ 		       " have been removed from the search path")
+        },
+        pos = "CheckExEnv")
> assign("ptime", proc.time(), pos = "CheckExEnv")
> grDevices::postscript("mboost-Ex.ps")
> assign("par.postscript", graphics::par(no.readonly = TRUE), pos = "CheckExEnv")
> options(contrasts = c(unordered = "contr.treatment", ordered = "contr.poly"))
> options(warn = 1)    
> library('mboost')
Loading required package: modeltools
Loading required package: stats4
Loading required package: party
Loading required package: survival
Loading required package: splines

Attaching package: 'survival'


	The following object(s) are masked from package:modeltools :

	 cluster 

Loading required package: grid
Loading required package: coin
Loading required package: mvtnorm
Loading required package: zoo

Attaching package: 'zoo'


	The following object(s) are masked from package:base :

	 rapply 

Loading required package: sandwich
Loading required package: strucchange
Loading required package: vcd
Loading required package: MASS
Loading required package: colorspace
> 
> assign(".oldSearch", search(), pos = 'CheckExEnv')
> assign(".oldNS", loadedNamespaces(), pos = 'CheckExEnv')
> cleanEx(); nameEx("FP");
> ### * FP
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FP
> ### Title: Fractional Polynomials
> ### Aliases: FP
> ### Keywords: datagen
> 
> ### ** Examples
> 
> 
>     data("bodyfat", package = "mboost")
>     tbodyfat <- bodyfat
>  
>     ### map covariates into [1, 2]
>     indep <- names(tbodyfat)[-2]
>     tbodyfat[indep] <- lapply(bodyfat[indep], function(x) {
+         x <- x - min(x)
+         x / max(x) + 1
+     })
>  
>     ### generate formula
>     fpfm <- as.formula(paste("DEXfat ~ ", paste("FP(", indep, ")", 
+                              collapse = "+")))
>     fpfm
DEXfat ~ FP(age) + FP(waistcirc) + FP(hipcirc) + FP(elbowbreadth) + 
    FP(kneebreadth) + FP(anthro3a) + FP(anthro3b) + FP(anthro3c) + 
    FP(anthro4)
> 
>     ### fit linear model
>     bf_fp <- glmboost(fpfm, data = tbodyfat, 
+                       control = boost_control(mstop = 3000))
> 
>     ### when to stop
>     mstop(aic <- AIC(bf_fp))
[1] 2480
>     plot(aic)
> 
>     ### coefficients
>     cf <- coef(bf_fp[mstop(aic)])
>     length(cf)
[1] 145
>     cf[abs(cf) > 0]
                                FP(age)age^-2 
                                  -1.97698853 
                         FP(age)log(age)age^3 
                                  -0.01230829 
                    FP(waistcirc)waistcirc^-2 
                                  -7.86296964 
      FP(waistcirc)log(waistcirc)waistcirc^-2 
                                  -7.47556696 
       FP(waistcirc)log(waistcirc)waistcirc^3 
                                   0.67466523 
                FP(waistcirc)log(waistcirc)^2 
                                   5.40114493 
                        FP(hipcirc)hipcirc^-2 
                                  -4.00222773 
            FP(hipcirc)log(hipcirc)hipcirc^-2 
                                   6.09380842 
            FP(hipcirc)log(hipcirc)hipcirc^-1 
                                   6.07680354 
             FP(hipcirc)log(hipcirc)hipcirc^3 
                                  -0.45247988 
                    FP(hipcirc)log(hipcirc)^2 
                                  18.07362051 
              FP(elbowbreadth)elbowbreadth^-2 
                                   1.19027807 
FP(kneebreadth)log(kneebreadth)kneebreadth^-2 
                                  -1.42177852 
FP(kneebreadth)log(kneebreadth)kneebreadth^-1 
                                  -3.22763358 
 FP(kneebreadth)log(kneebreadth)kneebreadth^3 
                                   1.85814405 
                      FP(anthro3a)anthro3a^-2 
                                   0.16767775 
         FP(anthro3a)log(anthro3a)anthro3a^-2 
                                  -8.14937266 
          FP(anthro3a)log(anthro3a)anthro3a^3 
                                   0.90354249 
                      FP(anthro3b)anthro3b^-2 
                                  -0.13985229 
          FP(anthro3b)log(anthro3b)anthro3b^3 
                                   1.31511811 
                      FP(anthro3c)anthro3c^-2 
                                 -10.94522292 
> 
> 
> 
> 
> cleanEx(); nameEx("Family");
> ### * Family
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Family
> ### Title: Gradient Boosting Families
> ### Aliases: Family AdaExp Binomial GaussClass GaussReg Huber Laplace
> ###   Poisson CoxPH
> ### Keywords: models
> 
> ### ** Examples
> 
> 
>     Laplace()

	 Absolute Error 

Loss function: abs(y - f) 
 
> 
>     Family(ngradient = function(y, f) y - f, 
+            loss = function(y, f) (y - f)^2,
+            name = "My Gauss Variant")

	 My Gauss Variant 

Loss function: (y - f)^2 
 
> 
> 
> 
> 
> cleanEx(); nameEx("blackboost");
> ### * blackboost
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: blackboost
> ### Title: Gradient Boosting with Regression Trees
> ### Aliases: blackboost blackboost.formula blackboost.matrix blackboost_fit
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- blackboost(dist ~ speed, data = cars,
+                           control = boost_control(mstop = 50))
>     cars.gb

	 Tree-Based Gradient Boosting

Call:
blackboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 50))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 50 
Step size:  0.1 
Offset:  42.98 

> 
>     ### plot fit
>     plot(dist ~ speed, data = cars)
>     lines(cars$speed, predict(cars.gb), col = "red")
> 
> 
> 
> 
> cleanEx(); nameEx("bodyfat");
> ### * bodyfat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bodyfat
> ### Title: Prediction of Body Fat by Skinfold Thickness, Circumferences,
> ###   and Bone Breadths
> ### Aliases: bodyfat
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>     data("bodyfat", package = "mboost")
> 
>     ### final model proposed by Garcia et al. (2005)
>     fmod <- lm(DEXfat ~ hipcirc + anthro3a + kneebreadth, data = bodyfat)
>     coef(fmod)  
(Intercept)     hipcirc    anthro3a kneebreadth 
-75.2347840   0.5115264   8.9096375   1.9019904 
> 
> 
> 
> 
> cleanEx(); nameEx("boost.family-class");
> ### * boost.family-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: boost_family-class
> ### Title: Class "boost_family": Gradient Boosting Family
> ### Aliases: boost_family-class show,boost_family-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> 
>     Laplace()

	 Absolute Error 

Loss function: abs(y - f) 
 
> 
> 
> 
> 
> cleanEx(); nameEx("cvrisk");
> ### * cvrisk
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cvrisk
> ### Title: Cross-Validation
> ### Aliases: cvrisk
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>   data("bodyfat", package = "mboost")
>   
>   ### fit linear model to data
>   model <- glmboost(DEXfat ~ ., data = bodyfat, 
+                     control = boost_control(center = TRUE))
> 
>   ### AIC-based selection of number of boosting iterations
>   maic <- AIC(model)
>   maic
[1] 3.352738
Optimal number of boosting iterations: 45 
Degrees of freedom (for mstop = 45): 1.917234 
> 
>   ### inspect coefficient path and AIC-based stopping criterion
>   par(mai = par("mai") * c(1, 1, 1, 1.8))
>   plot(model)
>   abline(v = mstop(maic), col = "lightgray")
> 
>   ### 10-fold cross-validation
>   n <- nrow(bodyfat)
>   k <- 10
>   ntest <- floor(n / k)
>   cv10f <- matrix(c(rep(c(rep(0, ntest), rep(1, n)), k - 1), 
+                     rep(0, n * k - (k - 1) * (n + ntest))), nrow = n)
>   cvm <- cvrisk(model, folds = cv10f)
>   print(cvm)

	 Cross-validated Squared Error (Regression) 
	 glmboost.formula(formula = DEXfat ~ ., data = bodyfat, control = boost_control(center = TRUE)) 

      10       20       30       40       50       60       70       80 
38.82368 19.71075 14.63678 12.81819 12.23021 12.01648 12.05912 12.12549 
      90      100 
12.17647 12.25439 

	 Optimal number of boosting iterations: 60 
>   mstop(cvm)
[1] 60
>   plot(cvm)
> 
>   ### 25 bootstrap iterations
>   set.seed(290875)
>   bs25 <- rmultinom(25, n, rep(1, n)/n)
>   cvm <- cvrisk(model, folds = bs25)
>   print(cvm)

	 Cross-validated Squared Error (Regression) 
	 glmboost.formula(formula = DEXfat ~ ., data = bodyfat, control = boost_control(center = TRUE)) 

      10       20       30       40       50       60       70       80 
32.82252 17.19064 14.19508 13.58873 13.41781 13.40558 13.45537 13.53205 
      90      100 
13.59870 13.65847 

	 Optimal number of boosting iterations: 60 
>   mstop(cvm)
[1] 60
> 
>   layout(matrix(1:2, ncol = 2))
>   plot(cvm)
> 
>   ### trees
>   blackbox <- blackboost(DEXfat ~ ., data = bodyfat)
>   cvtree <- cvrisk(blackbox, folds = bs25)
Warning: no admissible split found
Warning: no admissible split found
Warning: no admissible split found
Warning: no admissible split found
Warning: no admissible split found
>   plot(cvtree)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx(); nameEx("gamboost");
> ### * gamboost
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamboost
> ### Title: Gradient Boosting with Component-wise Smoothing Splines
> ### Aliases: gamboost gamboost.formula gamboost.matrix gamboost_fit
> ### Keywords: models nonlinear
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- gamboost(dist ~ speed, data = cars, dfbase = 4, 
+                         control = boost_control(mstop = 50))
>     cars.gb

	 Generalized Additive Models Fitted via Gradient Boosting

Call:
gamboost.formula(formula = dist ~ speed, data = cars, dfbase = 4,     control = boost_control(mstop = 50))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 50 
Step size:  0.1 
Offset:  42.98 
Degree of freedom:  4 

>     AIC(cars.gb, method = "corrected")
[1] 6.590677
Optimal number of boosting iterations: 25 
Degrees of freedom (for mstop = 25): 4.563027 
> 
>     ### plot fit for mstop = 1, ..., 50
>     plot(dist ~ speed, data = cars)    
>     tmp <- sapply(1:mstop(AIC(cars.gb)), function(i)
+         lines(cars$speed, predict(cars.gb[i]), col = "red"))          
>     lines(cars$speed, predict(smooth.spline(cars$speed, cars$dist),
+                               cars$speed)$y, col = "green")
> 
>     ### artificial example: sinus transformation
>     x <- sort(runif(100)) * 10
>     y <- sin(x) + rnorm(length(x), sd = 0.25)
>     plot(x, y)
>     ### linear model
>     lines(x, fitted(lm(y ~ sin(x) - 1)), col = "red")
>     ### GAM
>     lines(x, fitted(gamboost(y ~ x - 1, 
+                     control = boost_control(mstop = 500))), 
+           col = "green")
> 
> 
> 
> 
> cleanEx(); nameEx("glmboost");
> ### * glmboost
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmboost
> ### Title: Gradient Boosting with Component-wise Linear Models
> ### Aliases: glmboost glmboost.formula glmboost.matrix glmboost_fit
> ###   plot.glmboost
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- glmboost(dist ~ speed, data = cars, 
+                         control = boost_control(mstop = 5000))
>     cars.gb

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 5000))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 5000 
Step size:  0.1 
Offset:  42.98 

Coefficients: 
(Intercept)       speed 
 -60.559044    3.932406 
attr(,"offset")
[1] 42.98

> 
>     ### coefficients should coincide
>     coef(cars.gb) + c(cars.gb$offset, 0)
(Intercept)       speed 
 -17.579044    3.932406 
attr(,"offset")
[1] 42.98
>     coef(lm(dist ~ speed, data = cars))
(Intercept)       speed 
 -17.579095    3.932409 
> 
>     ### plot fit
>     layout(matrix(1:2, ncol = 2))
>     plot(dist ~ speed, data = cars)
>     lines(cars$speed, predict(cars.gb), col = "red")
> 
>     ### alternative loss function: absolute loss
>     cars.gbl <- glmboost(dist ~ speed, data = cars, 
+                          control = boost_control(mstop = 5000), 
+                          family = Laplace())
>     cars.gbl

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 5000),     family = Laplace())


	 Absolute Error 

Loss function: abs(y - f) 
 

Number of boosting iterations: mstop = 5000 
Step size:  0.1 
Offset:  36 

Coefficients: 
(Intercept)       speed 
 -29.532000    2.096213 
attr(,"offset")
[1] 36

> 
>     coef(cars.gbl) + c(cars.gbl$offset, 0)
(Intercept)       speed 
   6.468000    2.096213 
attr(,"offset")
[1] 36
>     lines(cars$speed, predict(cars.gbl), col = "green")
> 
>     ### Huber loss with adaptive choice of delta
>     cars.gbh <- glmboost(dist ~ speed, data = cars, 
+                          control = boost_control(mstop = 5000), 
+                          family = Huber())
> 
>     lines(cars$speed, predict(cars.gbh), col = "blue")
>     legend("topleft", col = c("red", "green", "blue"), lty = 1,
+            legend = c("Gaussian", "Laplace", "Huber"), bty = "n")
> 
>     ### plot coefficient path of glmboost
>     par(mai = par("mai") * c(1, 1, 1, 2.5))
>     plot(cars.gb)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx(); nameEx("methods");
> ### * methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: methods
> ### Title: Methods for Gradient Boosting Objects
> ### Aliases: print.glmboost coef.glmboost print.gamboost AIC.gb predict.gb
> ###   mstop mstop.gbAIC mstop.gb mstop.cvrisk mstop.blackboost fitted.gb
> ###   logLik.gb
> ### Keywords: methods
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- glmboost(dist ~ speed, data = cars, 
+                         control = boost_control(mstop = 2000))
>     cars.gb

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 2000))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 2000 
Step size:  0.1 
Offset:  42.98 

Coefficients: 
(Intercept)       speed 
 -60.331204    3.918359 
attr(,"offset")
[1] 42.98

> 
>     ### initial number of boosting iterations
>     mstop(cars.gb)
[1] 2000
> 
>     ### AIC criterion
>     aic <- AIC(cars.gb, method = "corrected")
>     aic
[1] 6.555391
Optimal number of boosting iterations: 1549 
Degrees of freedom (for mstop = 1549): 1.986856 
> 
>     ### coefficients for optimal number of boosting iterations
>     coef(cars.gb[mstop(aic)])
(Intercept)       speed 
 -59.751114    3.882873 
attr(,"offset")
[1] 42.98
>     plot(cars$dist, predict(cars.gb[mstop(aic)]), 
+          ylim = range(cars$dist))
>     abline(a = 0, b = 1)
> 
> 
> 
> 
> cleanEx(); nameEx("wpbc");
> ### * wpbc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wpbc
> ### Title: Wisconsin Prognostic Breast Cancer Data
> ### Aliases: wpbc
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>     data("wpbc", package = "mboost")
> 
>     ### fit logistic regression model with 100 boosting iterations
>     coef(glmboost(status ~ ., data = wpbc[,colnames(wpbc) != "time"], 
+                   family = Binomial()))
        (Intercept)         mean_radius        mean_texture      mean_perimeter 
       0.0000000000        0.0000000000       -0.0002103627        0.0000000000 
          mean_area     mean_smoothness    mean_compactness      mean_concavity 
       0.0000000000        0.0000000000        0.0000000000        0.0000000000 
 mean_concavepoints       mean_symmetry     mean_fractaldim           SE_radius 
       0.0000000000        0.0000000000        0.0000000000        0.0000000000 
         SE_texture        SE_perimeter             SE_area       SE_smoothness 
      -0.0889990837        0.0000000000        0.0005205701        0.0000000000 
     SE_compactness        SE_concavity    SE_concavepoints         SE_symmetry 
       0.0000000000       -0.5714488090       -4.0893001991        0.0000000000 
      SE_fractaldim        worst_radius       worst_texture     worst_perimeter 
       0.0000000000        0.0000000000        0.0000000000        0.0000000000 
         worst_area    worst_smoothness   worst_compactness     worst_concavity 
       0.0001013691        0.0000000000        0.0000000000        0.0000000000 
worst_concavepoints      worst_symmetry    worst_fractaldim               tsize 
       0.0000000000       -0.1901055629        0.0000000000        0.0365868895 
             pnodes 
       0.0000000000 
attr(,"offset")
[1] -0.5835661
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cat("Time elapsed: ", proc.time() - get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  18.749 0.116 19.361 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
