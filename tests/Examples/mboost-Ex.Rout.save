
R version 2.9.0 (2009-04-17)
Copyright (C) 2009 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> ### * <HEADER>
> ###
> attach(NULL, name = "CheckExEnv")
> assign("nameEx",
+        local({
+ 	   s <- "__{must remake R-ex/*.R}__"
+            function(new) {
+                if(!missing(new)) s <<- new else s
+            }
+        }),
+        pos = "CheckExEnv")
> ## Add some hooks to label plot pages for base and grid graphics
> assign("base_plot_hook",
+        function() {
+            pp <- par(c("mfg","mfcol","oma","mar"))
+            if(all(pp$mfg[1:2] == c(1, pp$mfcol[2]))) {
+                outer <- (oma4 <- pp$oma[4]) > 0; mar4 <- pp$mar[4]
+                mtext(sprintf("help(\"%s\")", nameEx()), side = 4,
+                      line = if(outer)max(1, oma4 - 1) else min(1, mar4 - 1),
+                outer = outer, adj = 1, cex = .8, col = "orchid", las=3)
+            }
+        },
+        pos = "CheckExEnv")
> assign("grid_plot_hook",
+        function() {
+            grid::pushViewport(grid::viewport(width=grid::unit(1, "npc") -
+                               grid::unit(1, "lines"), x=0, just="left"))
+            grid::grid.text(sprintf("help(\"%s\")", nameEx()),
+                            x=grid::unit(1, "npc") + grid::unit(0.5, "lines"),
+                            y=grid::unit(0.8, "npc"), rot=90,
+                            gp=grid::gpar(col="orchid"))
+        },
+        pos = "CheckExEnv")
> setHook("plot.new",     get("base_plot_hook", pos = "CheckExEnv"))
> setHook("persp",        get("base_plot_hook", pos = "CheckExEnv"))
> setHook("grid.newpage", get("grid_plot_hook", pos = "CheckExEnv"))
> assign("cleanEx",
+        function(env = .GlobalEnv) {
+ 	   rm(list = ls(envir = env, all.names = TRUE), envir = env)
+            RNGkind("default", "default")
+ 	   set.seed(1)
+    	   options(warn = 1)
+ 	   .CheckExEnv <- as.environment("CheckExEnv")
+ 	   delayedAssign("T", stop("T used instead of TRUE"),
+ 		  assign.env = .CheckExEnv)
+ 	   delayedAssign("F", stop("F used instead of FALSE"),
+ 		  assign.env = .CheckExEnv)
+ 	   sch <- search()
+ 	   newitems <- sch[! sch %in% .oldSearch]
+ 	   for(item in rev(newitems))
+                eval(substitute(detach(item), list(item=item)))
+ 	   missitems <- .oldSearch[! .oldSearch %in% sch]
+ 	   if(length(missitems))
+ 	       warning("items ", paste(missitems, collapse=", "),
+ 		       " have been removed from the search path")
+        },
+        pos = "CheckExEnv")
> assign("ptime", proc.time(), pos = "CheckExEnv")
> ## at least one package changes these via ps.options(), so do this
> ## before loading the package.
> ## Use postscript as incomplete files may be viewable, unlike PDF.
> ## Choose a size that is close to on-screen devices, fix paper
> ps.options(width = 7, height = 7, paper = "a4", reset = TRUE)
> grDevices::postscript("mboost-Ex.ps")
> 
> assign("par.postscript", graphics::par(no.readonly = TRUE), pos = "CheckExEnv")
> options(contrasts = c(unordered = "contr.treatment", ordered = "contr.poly"))
> options(warn = 1)
> library('mboost')
Loading required package: modeltools
Loading required package: stats4
Loading required package: party
Loading required package: survival
Loading required package: splines
Loading required package: grid
Loading required package: coin
Loading required package: mvtnorm
Loading required package: zoo

Attaching package: 'zoo'


	The following object(s) are masked from package:base :

	 as.Date.numeric 

Loading required package: sandwich
Loading required package: strucchange
Loading required package: vcd
Loading required package: MASS
Loading required package: colorspace
> 
> assign(".oldSearch", search(), pos = 'CheckExEnv')
> assign(".oldNS", loadedNamespaces(), pos = 'CheckExEnv')
> cleanEx(); nameEx("FP")
> ### * FP
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: FP
> ### Title: Fractional Polynomials
> ### Aliases: FP
> ### Keywords: datagen
> 
> ### ** Examples
> 
> 
>     data("bodyfat", package = "mboost")
>     tbodyfat <- bodyfat
> 
>     ### map covariates into [1, 2]
>     indep <- names(tbodyfat)[-2]
>     tbodyfat[indep] <- lapply(bodyfat[indep], function(x) {
+         x <- x - min(x)
+         x / max(x) + 1
+     })
> 
>     ### generate formula
>     fpfm <- as.formula(paste("DEXfat ~ ", paste("FP(", indep, ")",
+                              collapse = "+")))
>     fpfm
DEXfat ~ FP(age) + FP(waistcirc) + FP(hipcirc) + FP(elbowbreadth) + 
    FP(kneebreadth) + FP(anthro3a) + FP(anthro3b) + FP(anthro3c) + 
    FP(anthro4)
> 
>     ### fit linear model
>     bf_fp <- glmboost(fpfm, data = tbodyfat,
+                       control = boost_control(mstop = 3000))
> 
>     ### when to stop
>     mstop(aic <- AIC(bf_fp))
[1] 2480
>     plot(aic)
> 
>     ### coefficients
>     cf <- coef(bf_fp[mstop(aic)])
>     length(cf)
[1] 145
>     cf[abs(cf) > 0]
                                FP(age)age^-2 
                                  -1.97698853 
                         FP(age)log(age)age^3 
                                  -0.01230829 
                    FP(waistcirc)waistcirc^-2 
                                  -7.86296964 
      FP(waistcirc)log(waistcirc)waistcirc^-2 
                                  -7.47556696 
       FP(waistcirc)log(waistcirc)waistcirc^3 
                                   0.67466523 
                FP(waistcirc)log(waistcirc)^2 
                                   5.40114493 
                        FP(hipcirc)hipcirc^-2 
                                  -4.00222773 
            FP(hipcirc)log(hipcirc)hipcirc^-2 
                                   6.09380842 
            FP(hipcirc)log(hipcirc)hipcirc^-1 
                                   6.07680354 
             FP(hipcirc)log(hipcirc)hipcirc^3 
                                  -0.45247988 
                    FP(hipcirc)log(hipcirc)^2 
                                  18.07362051 
              FP(elbowbreadth)elbowbreadth^-2 
                                   1.19027807 
FP(kneebreadth)log(kneebreadth)kneebreadth^-2 
                                  -1.42177852 
FP(kneebreadth)log(kneebreadth)kneebreadth^-1 
                                  -3.22763358 
 FP(kneebreadth)log(kneebreadth)kneebreadth^3 
                                   1.85814405 
                      FP(anthro3a)anthro3a^-2 
                                   0.16767775 
         FP(anthro3a)log(anthro3a)anthro3a^-2 
                                  -8.14937266 
          FP(anthro3a)log(anthro3a)anthro3a^3 
                                   0.90354249 
                      FP(anthro3b)anthro3b^-2 
                                  -0.13985229 
          FP(anthro3b)log(anthro3b)anthro3b^3 
                                   1.31511811 
                      FP(anthro3c)anthro3c^-2 
                                 -10.94522292 
> 
> 
> 
> 
> cleanEx(); nameEx("Family")
> ### * Family
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Family
> ### Title: Gradient Boosting Families
> ### Aliases: Family AdaExp Binomial GaussClass GaussReg Huber Laplace
> ###   Poisson CoxPH QuantReg
> ### Keywords: models
> 
> ### ** Examples
> 
> 
>     Laplace()

	 Absolute Error 

Loss function: abs(y - f) 
 
> 
>     Family(ngradient = function(y, f) y - f,
+            loss = function(y, f) (y - f)^2,
+            name = "My Gauss Variant")

	 My Gauss Variant 

Loss function: (y - f)^2 
 
> 
> 
> 
> 
> cleanEx(); nameEx("Westbc")
> ### * Westbc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: Westbc
> ### Title: Breast Cancer Gene Expression
> ### Aliases: Westbc
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>     ## Not run: 
> ##D         library("Biobase")
> ##D         data("Westbc", package = "mboost")
> ##D         westbc <- new("ExpressionSet", 
> ##D               phenoData = new("AnnotatedDataFrame", data = Westbc$pheno),
> ##D               assayData = assayDataNew(exprs = Westbc$assay))
> ##D     
> ## End(Not run)
> 
> 
> 
> cleanEx(); nameEx("baselearners")
> ### * baselearners
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: baselearners
> ### Title: Base learners for Gradient Boosting with Smooth Components
> ### Aliases: bols bbs bns bss bspatial brandom btree
> ### Keywords: models
> 
> ### ** Examples
> 
> x1 <- rnorm(100)
> x2 <- rnorm(100) + 0.25*x1
> x3 <- as.factor(sample(0:1, 100, replace = TRUE))
> x4 <- gl(4, 25)
> y <- 3*sin(x1) + x2^2 + rnorm(100)
> 
> knots.x2 <- quantile(x2, c(0.25,0.5,0.75))
> 
> spline1 <- bbs(x1,knots=20,df=4)
> attributes(spline1)
$dim
[1] 100   1

$dpp
function (weights) 
{
    if (any(!cc)) 
        weights <- weights[cc]
    boundary.knots <- range(x[cc], na.rm = TRUE)
    bnw <- range(x[cc][weights > 0], na.rm = TRUE)
    if (!isTRUE(all.equal(boundary.knots, bnw))) 
        warning("knots (and therefore model) depend on observations with zero weight")
    if (length(knots) == 1) {
        knots <- seq(from = boundary.knots[1], to = boundary.knots[2], 
            length = knots + 2)
        knots <- knots[2:(length(knots) - 1)]
    }
    newX <- function(x, z = NULL, weights = NULL, na.rm = TRUE) {
        if (na.rm) {
            x <- x[cc]
            if (!is.null(z)) 
                z <- z[cc]
            if (!is.null(weights)) 
                weights <- weights[cc]
        }
        X <- bs(x, knots = knots, degree = degree, intercept = TRUE, 
            Boundary.knots = boundary.knots)
        if (!is.null(z)) 
            X <- X * z
        if (center) {
            K <- diff(diag(ncol(X)), differences = differences)
            X <- tcrossprod(X, K) %*% solve(tcrossprod(K))
        }
        return(X)
    }
    X <- newX(x, z, weights = weights)
    Xna <- X
    if (any(!cc)) 
        Xna <- newX(x, z, weights = weights, na.rm = FALSE)
    if (center) {
        K <- diag(ncol(X))
    }
    else {
        K <- diff(diag(ncol(X)), differences = differences)
        K <- crossprod(K, K)
    }
    lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    fitfun <- function(y) {
        if (any(!cc)) 
            y <- y[cc]
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(Xna %*% coef)
            nX <- newX(x = newdata[[xname]], z = newdata[[zname]], 
                na.rm = FALSE)
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() Xna %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xa884608>

> spline2 <- bns(x2,knots=knots.x2,df=5)
Warning in bns(x2, knots = knots.x2, df = 5) :
  non-equidistant ‘knots’ might be inappropriate
> attributes(spline2)
$dim
[1] 100   5

$dimnames
$dimnames[[1]]
NULL

$dimnames[[2]]
[1] "1" "2" "3" "4" "5"


$degree
[1] 3

$knots
       25%        50%        75% 
-0.7239450 -0.1837523  0.6988490 

$Boundary.knots
[1] -2.393725  2.865090

$intercept
[1] TRUE

$class
[1] "ns"    "basis"

$dpp
function (weights) 
{
    lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    fitfun <- function(y) {
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(X %*% coef)
            nX <- newX(x = newdata[[xname]], z = newdata[[zname]])
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() X %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xa8bb7e8>

> olsfit <- bols(x3)
> attributes(olsfit)
$dim
[1] 100   2

$dimnames
$dimnames[[1]]
  [1] "1"   "2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12" 
 [13] "13"  "14"  "15"  "16"  "17"  "18"  "19"  "20"  "21"  "22"  "23"  "24" 
 [25] "25"  "26"  "27"  "28"  "29"  "30"  "31"  "32"  "33"  "34"  "35"  "36" 
 [37] "37"  "38"  "39"  "40"  "41"  "42"  "43"  "44"  "45"  "46"  "47"  "48" 
 [49] "49"  "50"  "51"  "52"  "53"  "54"  "55"  "56"  "57"  "58"  "59"  "60" 
 [61] "61"  "62"  "63"  "64"  "65"  "66"  "67"  "68"  "69"  "70"  "71"  "72" 
 [73] "73"  "74"  "75"  "76"  "77"  "78"  "79"  "80"  "81"  "82"  "83"  "84" 
 [85] "85"  "86"  "87"  "88"  "89"  "90"  "91"  "92"  "93"  "94"  "95"  "96" 
 [97] "97"  "98"  "99"  "100"

$dimnames[[2]]
[1] "(Intercept)" "x1"         


$assign
[1] 0 1

$contrasts
$contrasts$x
[1] "contr.treatment"


$dpp
function (weights) 
{
    if (any(!cc)) 
        weights <- weights[cc]
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    if (is.null(df) || df >= ncol(K)) {
        Xsolve <- tcrossprod(solve(XtX), Xw)
    }
    else {
        lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
        Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    }
    fitfun <- function(y) {
        if (any(!cc)) 
            y <- y[cc]
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(Xna %*% coef)
            nX <- newX(x = newdata[[xname]], z = newdata[[zname]], 
                na.rm = FALSE)
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() Xna %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xa99b398>

> 
> form1 <- y ~ bbs(x1,knots=20,df=4) + bns(x2,knots=knots.x2,df=5)
> 
> # example for factors
> attributes(bols(x4))
$dim
[1] 100   4

$dimnames
$dimnames[[1]]
  [1] "1"   "2"   "3"   "4"   "5"   "6"   "7"   "8"   "9"   "10"  "11"  "12" 
 [13] "13"  "14"  "15"  "16"  "17"  "18"  "19"  "20"  "21"  "22"  "23"  "24" 
 [25] "25"  "26"  "27"  "28"  "29"  "30"  "31"  "32"  "33"  "34"  "35"  "36" 
 [37] "37"  "38"  "39"  "40"  "41"  "42"  "43"  "44"  "45"  "46"  "47"  "48" 
 [49] "49"  "50"  "51"  "52"  "53"  "54"  "55"  "56"  "57"  "58"  "59"  "60" 
 [61] "61"  "62"  "63"  "64"  "65"  "66"  "67"  "68"  "69"  "70"  "71"  "72" 
 [73] "73"  "74"  "75"  "76"  "77"  "78"  "79"  "80"  "81"  "82"  "83"  "84" 
 [85] "85"  "86"  "87"  "88"  "89"  "90"  "91"  "92"  "93"  "94"  "95"  "96" 
 [97] "97"  "98"  "99"  "100"

$dimnames[[2]]
[1] "(Intercept)" "x2"          "x3"          "x4"         


$assign
[1] 0 1 1 1

$contrasts
$contrasts$x
[1] "contr.treatment"


$dpp
function (weights) 
{
    if (any(!cc)) 
        weights <- weights[cc]
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    if (is.null(df) || df >= ncol(K)) {
        Xsolve <- tcrossprod(solve(XtX), Xw)
    }
    else {
        lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
        Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    }
    fitfun <- function(y) {
        if (any(!cc)) 
            y <- y[cc]
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(Xna %*% coef)
            nX <- newX(x = newdata[[xname]], z = newdata[[zname]], 
                na.rm = FALSE)
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() Xna %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xa8f53d4>

> 
> # example for bspatial
> 
> x1 <- runif(250,-pi,pi)
> x2 <- runif(250,-pi,pi)
> 
> y <- sin(x1)*sin(x2) + rnorm(250, sd = 0.4)
> 
> spline3 <- bspatial(x1, x2, xknots=12, yknots=12)
> attributes(spline3)
$dim
[1] 250   1

$dpp
function (weights) 
{
    if (any(!cc)) 
        weights <- weights[cc]
    xboundary.knots <- range(x[cc], na.rm = TRUE)
    xbnw <- range(x[cc][weights > 0], na.rm = TRUE)
    if (!isTRUE(all.equal(xboundary.knots, xbnw))) 
        warning("knots (and therefore model) depend on observations with zero weight")
    if (length(xknots) == 1) {
        xknots <- seq(from = xboundary.knots[1], to = xboundary.knots[2], 
            length = xknots + 2)
        xknots <- xknots[2:(length(xknots) - 1)]
    }
    yboundary.knots <- range(y[cc], na.rm = TRUE)
    ybnw <- range(y[cc][weights > 0], na.rm = TRUE)
    if (!isTRUE(all.equal(yboundary.knots, ybnw))) 
        warning("knots (and therefore model) depend on observations with zero weight")
    if (length(yknots) == 1) {
        yknots <- seq(from = yboundary.knots[1], to = yboundary.knots[2], 
            length = yknots + 2)
        yknots <- yknots[2:(length(yknots) - 1)]
    }
    newX <- function(x, y, z = NULL, weights = NULL, na.rm = TRUE) {
        if (na.rm) {
            x <- x[cc]
            y <- y[cc]
            if (!is.null(z)) 
                z <- z[cc]
            if (!is.null(weights)) 
                weights <- weights[cc]
        }
        Xx <- bs(x, knots = xknots, degree = degree, intercept = TRUE, 
            Boundary.knots = xboundary.knots)
        Xy <- bs(y, knots = yknots, degree = degree, intercept = TRUE, 
            Boundary.knots = yboundary.knots)
        X <- kronecker(Xx, matrix(1, nc = ncol(Xy))) * kronecker(matrix(1, 
            nc = ncol(Xx)), Xy)
        if (!is.null(z)) 
            X <- X * z
        if (center) {
            X <- X %*% L
        }
        return(X)
    }
    xd <- length(xknots) + degree + 1
    yd <- length(yknots) + degree + 1
    Kx <- diff(diag(xd), differences = differences)
    Kx <- crossprod(Kx, Kx)
    Ky <- diff(diag(yd), differences = differences)
    Ky <- crossprod(Ky, Ky)
    K <- kronecker(Kx, diag(yd)) + kronecker(diag(xd), Ky)
    if (center) {
        L <- 0
        L <- eigen(K, symmetric = TRUE, EISPACK = TRUE)
        L$vectors <- L$vectors[, 1:(xd * yd - differences^2)]
        L$values <- sqrt(L$values[1:(xd * yd - differences^2)])
        L <- L$vectors %*% diag(1/L$values)
        K <- diag(ncol(L))
    }
    X <- newX(x, y, z)
    Xna <- X
    if (any(!cc)) 
        Xna <- newX(x, y, z, weights = weights, na.rm = FALSE)
    lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    fitfun <- function(y) {
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(Xna %*% coef)
            nX <- newX(x = newdata[[xname]], y = newdata[[yname]], 
                z = newdata[[zname]], na.rm = FALSE)
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() Xna %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xa9b39ac>

> 
> form2 <- y ~ bspatial(x1, x2, xknots=12, yknots=12)
> 
> # decompose spatial effect into parametric part and deviation with 1 df
> 
> form2 <- y ~ bols(x1) + bols(x2) + bols(x1*x2) +
+              bspatial(x1, x2, xknots=12, yknots=12, center = TRUE, df=1)
> 
> # random intercept
> 
> id <- factor(rep(1:10, each=5))
> raneff <- brandom(id)
> attributes(raneff)
$dim
[1] 50 10

$dimnames
$dimnames[[1]]
 [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11" "12" "13" "14" "15"
[16] "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30"
[31] "31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44" "45"
[46] "46" "47" "48" "49" "50"

$dimnames[[2]]
 [1] "x1"  "x2"  "x3"  "x4"  "x5"  "x6"  "x7"  "x8"  "x9"  "x10"


$assign
 [1] 1 1 1 1 1 1 1 1 1 1

$contrasts
$contrasts$x
[1] "contr.treatment"


$dpp
function (weights) 
{
    lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    fitfun <- function(y) {
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(X %*% coef)
            nX <- newX(x = newdata[[xname]], z = newdata[[zname]])
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() X %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xa52a8a8>

> 
> # random slope
> 
> z <- runif(50)
> raneff <- brandom(id, z=z)
> attributes(raneff)
$assign
 [1] 1 1 1 1 1 1 1 1 1 1

$contrasts
$contrasts$x
[1] "contr.treatment"


$dim
[1] 50 10

$dimnames
$dimnames[[1]]
 [1] "1"  "2"  "3"  "4"  "5"  "6"  "7"  "8"  "9"  "10" "11" "12" "13" "14" "15"
[16] "16" "17" "18" "19" "20" "21" "22" "23" "24" "25" "26" "27" "28" "29" "30"
[31] "31" "32" "33" "34" "35" "36" "37" "38" "39" "40" "41" "42" "43" "44" "45"
[46] "46" "47" "48" "49" "50"

$dimnames[[2]]
 [1] "x1"  "x2"  "x3"  "x4"  "x5"  "x6"  "x7"  "x8"  "x9"  "x10"


$dpp
function (weights) 
{
    lambda <- df2lambda(X, df = df, dmat = K, weights = weights)
    Xw <- X * weights
    XtX <- crossprod(Xw, X)
    Xsolve <- tcrossprod(solve(XtX + lambda * K), Xw)
    fitfun <- function(y) {
        coef <- Xsolve %*% y
        predictfun <- function(newdata = NULL) {
            if (is.null(newdata)) 
                return(X %*% coef)
            nX <- newX(x = newdata[[xname]], z = newdata[[zname]])
            nX %*% coef
        }
        ret <- list(model = coef, predict = predictfun, fitted = function() X %*% 
            coef)
        class(ret) <- c("basefit", "baselm")
        ret
    }
    ret <- list(fit = fitfun, hatmatrix = function() X %*% Xsolve)
    class(ret) <- "basisdpp"
    ret
}
<environment: 0xae43cf8>

> 
> # remove intercept from base learner
> # and add explicit intercept to the model
> 
> tmpdata <- data.frame(x = 1:100, y = rnorm(1:100), int = rep(1, 100))
> mod <- gamboost(y ~ bols(int, center = TRUE) + bols(x, center = TRUE),
+                 data = tmpdata, control = boost_control(mstop = 2500))
> cf <- unlist(coef(mod))
> cf[1] <- cf[1] + mod$offset
> cf
bols(int, center = TRUE).x   bols(x, center = TRUE).x 
               0.239120107               -0.004211271 
> coef(lm(y ~ x, data = tmpdata))
 (Intercept)            x 
 0.239120133 -0.004211272 
> 
> 
> 
> 
> cleanEx(); nameEx("blackboost")
> ### * blackboost
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: blackboost
> ### Title: Gradient Boosting with Regression Trees
> ### Aliases: blackboost blackboost.formula blackboost.matrix blackboost_fit
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- blackboost(dist ~ speed, data = cars,
+                           control = boost_control(mstop = 50))
>     cars.gb

	 Tree-Based Gradient Boosting

Call:
blackboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 50))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 50 
Step size:  0.1 
Offset:  42.98 

> 
>     ### plot fit
>     plot(dist ~ speed, data = cars)
>     lines(cars$speed, predict(cars.gb), col = "red")
> 
> 
> 
> 
> cleanEx(); nameEx("bodyfat")
> ### * bodyfat
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bodyfat
> ### Title: Prediction of Body Fat by Skinfold Thickness, Circumferences,
> ###   and Bone Breadths
> ### Aliases: bodyfat
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>     data("bodyfat", package = "mboost")
> 
>     ### final model proposed by Garcia et al. (2005)
>     fmod <- lm(DEXfat ~ hipcirc + anthro3a + kneebreadth, data = bodyfat)
>     coef(fmod)  
(Intercept)     hipcirc    anthro3a kneebreadth 
-75.2347840   0.5115264   8.9096375   1.9019904 
> 
>     ### plot additive model for same variables
>     amod <- gamboost(DEXfat ~ hipcirc + anthro3a + kneebreadth, 
+                      data = bodyfat, baselearner = "bbs")
>     layout(matrix(1:3, ncol = 3))
>     plot(amod[mstop(AIC(amod, "corrected"))], ask = FALSE)
> 
> 
> 
> 
> cleanEx(); nameEx("boost_family-class")
> ### * boost_family-class
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: boost_family-class
> ### Title: Class "boost_family": Gradient Boosting Family
> ### Aliases: boost_family-class show,boost_family-method
> ### Keywords: classes
> 
> ### ** Examples
> 
> 
>     Laplace()

	 Absolute Error 

Loss function: abs(y - f) 
 
> 
> 
> 
> 
> cleanEx(); nameEx("cvrisk")
> ### * cvrisk
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: cvrisk
> ### Title: Cross-Validation
> ### Aliases: cvrisk
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>   data("bodyfat", package = "mboost")
> 
>   ### fit linear model to data
>   model <- glmboost(DEXfat ~ ., data = bodyfat,
+                     control = boost_control(center = TRUE))
> 
>   ### AIC-based selection of number of boosting iterations
>   maic <- AIC(model)
>   maic
[1] 3.352738
Optimal number of boosting iterations: 45 
Degrees of freedom (for mstop = 45): 1.917234 
> 
>   ### inspect coefficient path and AIC-based stopping criterion
>   par(mai = par("mai") * c(1, 1, 1, 1.8))
>   plot(model)
>   abline(v = mstop(maic), col = "lightgray")
> 
>   ### 10-fold cross-validation
>   n <- nrow(bodyfat)
>   k <- 10
>   ntest <- floor(n / k)
>   cv10f <- matrix(c(rep(c(rep(0, ntest), rep(1, n)), k - 1),
+                     rep(0, n * k - (k - 1) * (n + ntest))), nrow = n)
>   cvm <- cvrisk(model, folds = cv10f)
Loading required package: multicore
>   print(cvm)

	 Cross-validated Squared Error (Regression) 
	 glmboost.formula(formula = DEXfat ~ ., data = bodyfat, control = boost_control(center = TRUE)) 

        1         2         3         4         5         6         7         8 
114.40523  98.06521  85.32971  73.92584  66.42888  58.58223  52.92717  48.11107 
        9        10        11        12        13        14        15        16 
 43.44177  38.82368  34.69010  32.43575  30.26747  27.69441  25.90334  24.56641 
       17        18        19        20        21        22        23        24 
 22.97080  21.83605  20.65550  19.71075  18.83332  18.12002  17.59685  16.87540 
       25        26        27        28        29        30        31        32 
 16.48051  16.18608  15.59530  15.32054  14.87441  14.63678  14.38958  14.12979 
       33        34        35        36        37        38        39        40 
 13.88300  13.68727  13.49912  13.31103  13.15311  13.03541  12.94123  12.81819 
       41        42        43        44        45        46        47        48 
 12.70313  12.66793  12.60082  12.51772  12.44364  12.34291  12.33709  12.26853 
       49        50        51        52        53        54        55        56 
 12.22757  12.23021  12.17159  12.18627  12.12249  12.09211  12.09981  12.09584 
       57        58        59        60        61        62        63        64 
 12.09701  12.05070  12.03800  12.01648  12.03630  12.04265  12.03306  12.03832 
       65        66        67        68        69        70        71        72 
 12.01891  12.02957  12.02650  12.03260  12.05505  12.05912  12.05548  12.06391 
       73        74        75        76        77        78        79        80 
 12.07102  12.07261  12.08883  12.08249  12.07888  12.08784  12.09927  12.12549 
       81        82        83        84        85        86        87        88 
 12.12826  12.13126  12.12796  12.14285  12.14607  12.15301  12.17057  12.17305 
       89        90        91        92        93        94        95        96 
 12.18268  12.17647  12.19278  12.20042  12.19723  12.21259  12.22006  12.22888 
       97        98        99       100 
 12.23484  12.23280  12.23929  12.25439 

	 Optimal number of boosting iterations: 60 
>   mstop(cvm)
[1] 60
>   plot(cvm)
> 
>   ### 25 bootstrap iterations
>   set.seed(290875)
>   bs25 <- rmultinom(25, n, rep(1, n)/n)
>   cvm <- cvrisk(model, folds = bs25)
>   print(cvm)

	 Cross-validated Squared Error (Regression) 
	 glmboost.formula(formula = DEXfat ~ ., data = bodyfat, control = boost_control(center = TRUE)) 

        1         2         3         4         5         6         7         8 
105.21681  90.39822  78.01859  67.62284  59.28276  51.69324  45.81663  40.72354 
        9        10        11        12        13        14        15        16 
 36.18268  32.82252  29.68052  27.34685  25.14741  23.32195  21.86297  20.62092 
       17        18        19        20        21        22        23        24 
 19.51134  18.57217  17.82785  17.19064  16.61903  16.14986  15.75519  15.42828 
       25        26        27        28        29        30        31        32 
 15.12715  14.86865  14.67645  14.47705  14.32909  14.19508  14.09503  14.02636 
       33        34        35        36        37        38        39        40 
 13.91859  13.85153  13.81214  13.73382  13.69756  13.66757  13.62300  13.58873 
       41        42        43        44        45        46        47        48 
 13.54895  13.53668  13.50329  13.49420  13.46374  13.45288  13.43445  13.44060 
       49        50        51        52        53        54        55        56 
 13.42223  13.41781  13.42916  13.41414  13.40484  13.40567  13.40934  13.40576 
       57        58        59        60        61        62        63        64 
 13.41607  13.41409  13.40710  13.40558  13.41583  13.41963  13.43354  13.42803 
       65        66        67        68        69        70        71        72 
 13.42735  13.42628  13.44171  13.45126  13.45371  13.45537  13.45598  13.46645 
       73        74        75        76        77        78        79        80 
 13.48861  13.47666  13.48980  13.50234  13.50078  13.51064  13.51178  13.53205 
       81        82        83        84        85        86        87        88 
 13.53838  13.53504  13.54260  13.54861  13.56030  13.56971  13.58088  13.58164 
       89        90        91        92        93        94        95        96 
 13.58812  13.59870  13.60281  13.61136  13.61502  13.61518  13.62406  13.62941 
       97        98        99       100 
 13.64013  13.64593  13.65394  13.65847 

	 Optimal number of boosting iterations: 53 
>   mstop(cvm)
[1] 53
> 
>   layout(matrix(1:2, ncol = 2))
>   plot(cvm)
> 
>   ### trees
>   blackbox <- blackboost(DEXfat ~ ., data = bodyfat)
>   cvtree <- cvrisk(blackbox, folds = bs25)
>   plot(cvtree)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx(); nameEx("gamboost")
> ### * gamboost
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: gamboost
> ### Title: Gradient Boosting with Smooth Components
> ### Aliases: gamboost gamboost.formula gamboost.matrix gamboost_fit
> ###   plot.gamboost
> ### Keywords: models nonlinear
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- gamboost(dist ~ speed, data = cars, dfbase = 4,
+                         control = boost_control(mstop = 50))
>     cars.gb

	 Generalized Additive Models Fitted via Gradient Boosting

Call:
gamboost.formula(formula = dist ~ speed, data = cars, dfbase = 4,     control = boost_control(mstop = 50))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 50 
Step size:  0.1 
Offset:  42.98 

>     AIC(cars.gb, method = "corrected")
[1] 6.60797
Optimal number of boosting iterations: 24 
Degrees of freedom (for mstop = 24): 4.481722 
> 
>     ### plot fit for mstop = 1, ..., 50
>     plot(dist ~ speed, data = cars)
>     tmp <- sapply(1:mstop(AIC(cars.gb)), function(i)
+         lines(cars$speed, predict(cars.gb[i]), col = "red"))
>     lines(cars$speed, predict(smooth.spline(cars$speed, cars$dist),
+                               cars$speed)$y, col = "green")
> 
>     ### artificial example: sinus transformation
>     x <- sort(runif(100)) * 10
>     y <- sin(x) + rnorm(length(x), sd = 0.25)
>     plot(x, y)
>     ### linear model
>     lines(x, fitted(lm(y ~ sin(x) - 1)), col = "red")
>     ### GAM
>     lines(x, fitted(gamboost(y ~ x - 1,
+                     control = boost_control(mstop = 500))),
+           col = "green")
> 
> 
> 
> 
> cleanEx(); nameEx("glmboost")
> ### * glmboost
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: glmboost
> ### Title: Gradient Boosting with Component-wise Linear Models
> ### Aliases: glmboost glmboost.formula glmboost.matrix glmboost_fit
> ###   plot.glmboost
> ### Keywords: models regression
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- glmboost(dist ~ speed, data = cars,
+                         control = boost_control(mstop = 5000))
>     cars.gb

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 5000))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 5000 
Step size:  0.1 
Offset:  42.98 

Coefficients: 
(Intercept)       speed 
 -60.559044    3.932406 
attr(,"offset")
[1] 42.98

> 
>     ### coefficients should coincide
>     coef(cars.gb) + c(cars.gb$offset, 0)
(Intercept)       speed 
 -17.579044    3.932406 
attr(,"offset")
[1] 42.98
>     coef(lm(dist ~ speed, data = cars))
(Intercept)       speed 
 -17.579095    3.932409 
> 
>     ### plot fit
>     layout(matrix(1:2, ncol = 2))
>     plot(dist ~ speed, data = cars)
>     lines(cars$speed, predict(cars.gb), col = "red")
> 
>     ### alternative loss function: absolute loss
>     cars.gbl <- glmboost(dist ~ speed, data = cars,
+                          control = boost_control(mstop = 5000),
+                          family = Laplace())
>     cars.gbl

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 5000),     family = Laplace())


	 Absolute Error 

Loss function: abs(y - f) 
 

Number of boosting iterations: mstop = 5000 
Step size:  0.1 
Offset:  36 

Coefficients: 
(Intercept)       speed 
 -29.532000    2.096213 
attr(,"offset")
[1] 36

> 
>     coef(cars.gbl) + c(cars.gbl$offset, 0)
(Intercept)       speed 
   6.468000    2.096213 
attr(,"offset")
[1] 36
>     lines(cars$speed, predict(cars.gbl), col = "green")
> 
>     ### Huber loss with adaptive choice of delta
>     cars.gbh <- glmboost(dist ~ speed, data = cars,
+                          control = boost_control(mstop = 5000),
+                          family = Huber())
> 
>     lines(cars$speed, predict(cars.gbh), col = "blue")
>     legend("topleft", col = c("red", "green", "blue"), lty = 1,
+            legend = c("Gaussian", "Laplace", "Huber"), bty = "n")
> 
>     ### plot coefficient path of glmboost
>     par(mai = par("mai") * c(1, 1, 1, 2.5))
>     plot(cars.gb)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> cleanEx(); nameEx("methods")
> ### * methods
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: methods
> ### Title: Methods for Gradient Boosting Objects
> ### Aliases: print.glmboost coef.glmboost coef.gamboost print.gamboost
> ###   AIC.gamboost AIC.glmboost predict.gb predict.blackboost mstop
> ###   mstop.gbAIC mstop.gb mstop.cvrisk mstop.blackboost fitted.gb
> ###   logLik.gb
> ### Keywords: methods
> 
> ### ** Examples
> 
> 
>     ### a simple two-dimensional example: cars data
>     cars.gb <- glmboost(dist ~ speed, data = cars,
+                         control = boost_control(mstop = 2000))
>     cars.gb

	 Generalized Linear Models Fitted via Gradient Boosting

Call:
glmboost.formula(formula = dist ~ speed, data = cars, control = boost_control(mstop = 2000))


	 Squared Error (Regression) 

Loss function: (y - f)^2 
 

Number of boosting iterations: mstop = 2000 
Step size:  0.1 
Offset:  42.98 

Coefficients: 
(Intercept)       speed 
 -60.331204    3.918359 
attr(,"offset")
[1] 42.98

> 
>     ### initial number of boosting iterations
>     mstop(cars.gb)
[1] 2000
> 
>     ### AIC criterion
>     aic <- AIC(cars.gb, method = "corrected")
>     aic
[1] 6.555391
Optimal number of boosting iterations: 1549 
Degrees of freedom (for mstop = 1549): 1.986856 
> 
>     ### coefficients for optimal number of boosting iterations
>     coef(cars.gb[mstop(aic)])
(Intercept)       speed 
 -59.751114    3.882873 
attr(,"offset")
[1] 42.98
>     plot(cars$dist, predict(cars.gb[mstop(aic)]),
+          ylim = range(cars$dist))
>     abline(a = 0, b = 1)
> 
> 
> 
> 
> cleanEx(); nameEx("survFit")
> ### * survFit
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: survFit
> ### Title: Survival Curves for a Cox Proportional Hazards Model
> ### Aliases: survFit survFit.gb survFit.blackboost plot.survFit
> 
> 
> ### ** Examples
> 
> fm <- Surv(futime,fustat) ~ age + resid.ds + rx + ecog.ps
> fit <- glmboost(fm, data = ovarian, family = CoxPH(),
+     control=boost_control(mstop = 500))
> 
> S1 <- survFit(fit)
> S1
$surv
           [,1]
 [1,] 0.9658532
 [2,] 0.9301833
 [3,] 0.8925100
 [4,] 0.8535429
 [5,] 0.8119403
 [6,] 0.7708223
 [7,] 0.7297132
 [8,] 0.6826960
 [9,] 0.6325709
[10,] 0.5820681
[11,] 0.5218497
[12,] 0.4633558

$time
 [1]  59 115 156 268 329 353 365 431 464 475 563 638

$n.event
 [1] 1 1 1 1 1 1 1 1 1 1 1 1

attr(,"class")
[1] "survFit"
> newdata <- ovarian[c(1,3,12),]
> S2 <- survFit(fit, newdata = newdata)
> S2
$surv
              1         3        12
 [1,] 0.9261555 0.9398046 0.9786631
 [2,] 0.8523142 0.8786884 0.9560666
 [3,] 0.7779547 0.8161129 0.9318412
 [4,] 0.7049307 0.7535397 0.9063720
 [5,] 0.6312922 0.6891727 0.8786885
 [6,] 0.5628548 0.6280552 0.8507935
 [7,] 0.4987027 0.5694634 0.8223344
 [8,] 0.4305029 0.5055683 0.7890285
 [9,] 0.3637907 0.4411647 0.7525473
[10,] 0.3027367 0.3802158 0.7146640
[11,] 0.2378723 0.3128096 0.6678203
[12,] 0.1829547 0.2529419 0.6203097

$time
 [1]  59 115 156 268 329 353 365 431 464 475 563 638

$n.event
 [1] 1 1 1 1 1 1 1 1 1 1 1 1

attr(,"class")
[1] "survFit"
> 
> plot(S1)
> 
> 
> 
> cleanEx(); nameEx("wpbc")
> ### * wpbc
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: wpbc
> ### Title: Wisconsin Prognostic Breast Cancer Data
> ### Aliases: wpbc
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
>     data("wpbc", package = "mboost")
> 
>     ### fit logistic regression model with 100 boosting iterations
>     coef(glmboost(status ~ ., data = wpbc[,colnames(wpbc) != "time"], 
+                   family = Binomial()))
        (Intercept)         mean_radius        mean_texture      mean_perimeter 
       0.000000e+00        0.000000e+00       -6.138116e-04        0.000000e+00 
          mean_area     mean_smoothness    mean_compactness      mean_concavity 
       0.000000e+00        0.000000e+00        0.000000e+00        0.000000e+00 
 mean_concavepoints       mean_symmetry     mean_fractaldim           SE_radius 
       0.000000e+00        0.000000e+00        0.000000e+00        0.000000e+00 
         SE_texture        SE_perimeter             SE_area       SE_smoothness 
      -8.842521e-02        0.000000e+00        5.800290e-04        0.000000e+00 
     SE_compactness        SE_concavity    SE_concavepoints         SE_symmetry 
       0.000000e+00       -1.727113e+00       -3.711237e+00        0.000000e+00 
      SE_fractaldim        worst_radius       worst_texture     worst_perimeter 
       0.000000e+00        0.000000e+00        0.000000e+00        0.000000e+00 
         worst_area    worst_smoothness   worst_compactness     worst_concavity 
       9.995163e-05        0.000000e+00        0.000000e+00        0.000000e+00 
worst_concavepoints      worst_symmetry    worst_fractaldim               tsize 
       0.000000e+00        0.000000e+00        0.000000e+00        5.728343e-03 
             pnodes 
       2.225939e-02 
attr(,"offset")
[1] -0.5842854
> 
> 
> 
> 
> ### * <FOOTER>
> ###
> cat("Time elapsed: ", proc.time() - get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  9.668 0.12 10.946 0.964 0.964 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
