
R version 2.15.2 (2012-10-26) -- "Trick or Treat"
Copyright (C) 2012 The R Foundation for Statistical Computing
ISBN 3-900051-07-0
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> 
> library("mboost")
Loading required package: parallel
This is mboost 2.2-1. See 'package?mboost' and the NEWS file
for a complete list of changes.
Note: The default for the computation of the degrees of freedom has changed.
      For details see section 'Global Options' of '?bols'.
> attach(asNamespace("mboost"))
The following object(s) are masked from 'package:mboost':

    %+%, %O%, %X%, AUC, AdaExp, Binomial, CoxPH, ExpectReg, FP, Family,
    GammaReg, GaussClass, GaussReg, Gaussian, Gehan, Huber, IPCweights,
    Laplace, Loglog, Lognormal, NBinomial, Poisson, PropOdds, QuantReg,
    Weibull, bbs, blackboost, bmono, bmrf, bns, bols, boost_control,
    brad, brandom, bspatial, bss, btree, buser, cv, cvrisk, extract,
    gamboost, glmboost, mboost, mboost_fit, mstop, nuisance, selected,
    stabsel, survFit
> library("MASS")
> library("Matrix")
Loading required package: lattice
> 
> set.seed(290875)
> 
> ### dgp
> n <- 20000
> xn <- round(runif(n), 3)
> xn[sample(1:n)[1:(n / 100)]] <- NA
> xf <- gl(4, n / 4)
> xf[sample(1:n)[1:(n / 100)]] <- NA
> z1 <- sample(gl(2, n / 2))
> z1[sample(1:n)[1:(n / 100)]] <- NA
> z2 <- round(runif(n), 3)
> z2[sample(1:n)[1:(n / 100)]] <- NA
> w <- rpois(n, lambda = 2)
> y <- 2 * xn + rnorm(n)
> y[is.na(y)] <- rnorm(sum(is.na(y)))
> 
> testfun <- function(m1, m2) {
+     ret <- c(max(abs(coef(m1) - coef(m2))),
+       max(abs(fitted(m1) - fitted(m2)), na.rm = TRUE))
+     if (any(ret > sqrt(.Machine$double.eps)))
+         return(ret)
+ }
> 
> ### numeric x with intercept
> m1 <- lm(y ~ xn, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn), w), y)
Warning message:
In bols(xn) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### numeric x without intercept
> m1 <- lm(y ~ xn - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn, intercept = FALSE), w), y)
Warning messages:
1: In bols(xn, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
2: In bols(xn, intercept = FALSE) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### factor x with intercept
> m1 <- lm(y ~ xf, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xf), w), y)
Warning message:
In bols(xf) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### factor x without intercept
> tmp <- model.matrix(~ xf)[,-1] ## build model matrix without first row
> mm <- matrix(NA, ncol = ncol(tmp), nrow = length(y))
> mm[!is.na(xf),] <- tmp ## build model matrix with missings
> m1 <- lm(y ~ mm - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xf, intercept = FALSE), w), y)
Warning message:
In bols(xf, intercept = FALSE) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### factor x with "contr.dummy"
> m1 <- lm(y ~ xf - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xf, contrasts.arg = "contr.dummy"), w), y)
Warning message:
In bols(xf, contrasts.arg = "contr.dummy") :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### contrasts
> m1 <- lm(y ~ xf, weights = w, contrasts = list(xf = "contr.sum"), na.action = na.exclude)
> m2 <- fit(dpp(bols(xf, contrasts.arg = list(xf = "contr.sum")), w), y)
Warning message:
In bols(xf, contrasts.arg = list(xf = "contr.sum")) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### multiple x
> m1 <- lm(y ~ xn + xf, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn, xf), w), y)
Warning message:
In bols(xn, xf) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### interaction with binary factor
> xtmp <- (z1 == "2") * xn
> m1 <- lm(y ~ xtmp - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(xn, by = z1, intercept = FALSE), w), y)
Warning messages:
1: In bols(xn, by = z1, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
2: In bols(xn, by = z1, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### interaction with numeric variable
> m1 <- lm(y ~ z2:xn - 1, weights = w, na.action = na.exclude)
> m2 <- fit(dpp(bols(z2, by = xn, intercept = FALSE), w), y)
Warning messages:
1: In bols(z2, by = xn, intercept = FALSE) :
  covariates should be (mean-) centered if 'intercept = FALSE'
2: In bols(z2, by = xn, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> testfun(m1, m2)
> 
> ### ridge
> one <- rep(1, n)
> cf1 <- coef(lm.ridge(y ~ one + xn - 1, lambda = 2))
> cf2 <- coef(fit(dpp(bols(one, xn, lambda = 2, intercept = FALSE), rep(1, n)), y))
Warning message:
In bols(one, xn, lambda = 2, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> max(abs(cf1 - cf2))
[1] 0.001606092
> cf1 <- coef(lm.ridge(y ~ xf - 1, lambda = 2))
> cf2 <- coef(fit(dpp(bols(xf, lambda = 2, intercept = FALSE), rep(1, n)), y))
Warning message:
In bols(xf, lambda = 2, intercept = FALSE) :
  base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> max(abs(cf1 - cf2))
[1] 0.02597228
Warning message:
In cf1 - cf2 :
  longer object length is not a multiple of shorter object length
> 
> ### matrix (here with missing values)
> cf1 <- coef(mod <- lm(y ~ xn + xf * z1 - 1, weights = w, y = TRUE, x = TRUE))
> tX <- mod$x
> tw <- mod$weights
> ty <- mod$y
> cf2 <- coef(fit(dpp(bols(tX), weights = tw), ty))
> stopifnot(max(abs(cf1 - cf2)) < sqrt(.Machine$double.eps))
> 
> ### ridge again with matrix interface
> tX <- matrix(runif(1000), ncol = 10)
> ty <- rnorm(100)
> tw <- rep(1, 100)
> 
> ### compute & check df
> op <- options(mboost_dftraceS = TRUE)
> la <- df2lambda(tX, df = 2, dmat = diag(ncol(tX)), weights = tw)["lambda"]
> truedf <- sum(diag(tX %*% solve(crossprod(tX * tw, tX) + la * diag(ncol(tX))) %*% t(tX * tw)))
> stopifnot(abs(truedf - 2) < sqrt(.Machine$double.eps))
> 
> one <- rep(1, ncol(tX))
> cf1 <- coef(lm.ridge(ty ~ . - 1, data = as.data.frame(tX), lambda = la))
> cf2 <- coef(fit(dpp(bols(tX, df = 2), weights = tw), ty))
> max(abs(cf1 - cf2))
[1] 0.08707475
> # I think bols is better and thus right
> sum((ty - tX %*% cf1)^2) + la * sum(cf1^2)
  lambda 
80.20144 
> sum((ty - tX %*% cf2)^2) + la * sum(cf2^2)
  lambda 
78.29787 
> options(op)
> 
> ### now with other df-definition:
> op <- options(mboost_dftraceS = FALSE)
> la <- df2lambda(tX, df = 2, dmat = diag(ncol(tX)), weights = tw)["lambda"]
> H <- tX %*% solve(crossprod(tX * tw, tX) + la * diag(ncol(tX))) %*% t(tX * tw)
> truedf <- sum(diag(2*H - tcrossprod(H,H)))
> stopifnot(abs(truedf - 2) < sqrt(.Machine$double.eps))
> options(op)
> 
> # check df with weights
> op <- options(mboost_dftraceS = TRUE)
> tw <- rpois(100, 2)
> la <- df2lambda(tX, df = 2, dmat = diag(ncol(tX)), weights = tw)["lambda"]
> truedf <- sum(diag(tX %*% solve(crossprod(tX * tw, tX) + la * diag(ncol(tX))) %*% t(tX * tw)))
> stopifnot(abs(truedf - 2) < sqrt(.Machine$double.eps))
> 
> ### check df2lambda for P-splines (Bug spotted by B. Hofner)
> set.seed(1907)
> x <- runif(100, min = -1, max = 3)
> ## extract lambda from base-learner
> lambda <- bbs(x, df = 4)$dpp(rep(1, length(x)))$df()["lambda"]
> X <- get("X", envir = environment(bbs(x, df = 4)$dpp))
> K <- get("K", envir = environment(bbs(x, df = 4)$dpp))
> truedf <- sum(diag(X %*%  solve(crossprod(X,X) + lambda * K) %*% t(X)))
> stopifnot(abs(truedf - 4) < sqrt(.Machine$double.eps))
> 
> ### check accuracy of df2lambda
> data("bodyfat", package="mboost")
> diff_df <- matrix(NA, nrow=8, ncol=ncol(bodyfat))
> rownames(diff_df) <- paste("df", 3:10)
> colnames(diff_df) <- names(bodyfat)
> for (i in 3:10){
+     for (j in 1:ncol(bodyfat)){
+         lambda <- bbs(bodyfat[[j]], df = i)$dpp(rep(1, nrow(bodyfat)))$df()["lambda"]
+         diff_df[i-2,j] <- bbs(bodyfat[[j]], lambda = lambda)$dpp(rep(1, nrow(bodyfat)))$df()["df"] - i
+     }
+ }
> stopifnot(all(diff_df < sqrt(.Machine$double.eps)))
> options(op)
> 
> ### componentwise
> cf2 <- coef(fit(dpp(bolscw(cbind(1, xn)), weights = w), y))
> cf1 <- coef(lm(y ~ xn - 1, weights = w))
> stopifnot(max(abs(cf1 - max(cf2))) < sqrt(.Machine$double.eps))
> 
> cf2 <- coef(fit(dpp(bolscw(matrix(xn, nc = 1)), weights = w), y))
> cf1 <- coef(lm(y ~ xn - 1, weights = w))
> stopifnot(max(abs(cf1 - max(cf2))) < sqrt(.Machine$double.eps))
> 
> ### componentwise with matrix
> n <- 200
> m <- 10000
> x <- rnorm(n * m)
> x[abs(x) < 2] <- 0
> X <- Matrix(data = x, ncol = m, nrow = n)
> beta <- rpois(ncol(X), lambda = 1)
> y <- X %*% beta + rnorm(nrow(X))
> w <- rep(1, nrow(X)) ###rpois(nrow(X), lambda = 1)
> f1 <- dpp(bolscw(X), weights = w)$fit
> f1(y)$model
       coef     xselect           p 
   39.20852  3630.00000 10000.00000 
> 
> ### varying coefficients
> x1 <- runif(n, max = 2)
> x2 <- sort(runif(n, max = 2 * pi))
> y <- sin(x2) * x1 + rnorm(n)
> w <- rep(1, n)
> 
> d <- dpp(bbs(x2, by = x1, df = 4), w)
> f <- fit(d, y)
> f2 <- d$predict(list(f), newdata = data.frame(x1 = 1, x2 = x2))
> 
> max(abs(sin(x2) - f2))
[1] 0.7891727
> 
> ### bols and bbs; matrix interfaces
> n <- 10000
> x <- runif(n, min = 0, max = 2*pi)
> y <- sin(x) + rnorm(n, sd = 0.1)
> w <- rpois(n, lambda = 1)
> x[sample(1:n)[1:(n / 100)]] <- NA
> h <- hyper_bbs(data.frame(x = x), vary = "")
> X <- X_bbs(data.frame(x = x), vary = "", h)$X
> f1 <- fit(dpp(bbs(x, df = ncol(X)), w), y)
Warning message:
In bbs(x, df = ncol(X)) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> f2 <- fit(dpp(bols(X, df = ncol(X)), w), y)
Warning message:
In bols(X, df = ncol(X)) : base-learner contains missing values;
missing values are excluded per base-learner, i.e., base-learners may depend on different numbers of observations.
> stopifnot(max(abs(coef(f1) - coef(f2))) < sqrt(.Machine$double.eps))
> 
> stopifnot(all.equal(get_index(data.frame(x, x)), get_index(X)))
> stopifnot(all.equal(get_index(data.frame(x)), get_index(X)))
> 
> ### check handling of missings for cyclic effects
> h <- hyper_bbs(data.frame(x = x), vary = "", cyclic = TRUE)
> X <- X_bbs(data.frame(x = x), vary = "", h)$X
> stopifnot(all(is.na(X[is.na(x),])))
> stopifnot(all(!is.na(X[!is.na(x),])))
> 
> ### combinations and tensor products of base-learners
> 
> set.seed(29)
> n <- 1000
> x1 <- rnorm(n)
> x2 <- rnorm(n)
> x3 <- rnorm(n)
> f <- gl(4, 25)
> y <- rnorm(n)
> ndf <- data.frame(x1 = x1[1:10], x2 = x2[1:10], f = f[1:10])
> 
> ### spatial
> m1 <- gamboost(y ~ bbs(x1) %X% bbs(x2))
> m2 <- gamboost(y ~ bspatial(x1, x2, df = 16))
> stopifnot(max(abs(predict(m1) - predict(m2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(m1, newdata = ndf) - predict(m2, newdata = ndf))) < sqrt(.Machine$double.eps))
> 
> ### spatio-temporal
> m1 <- gamboost(y ~ bbs(x1, knots = 6) %X% bbs(x2, knots = 6) %X% bbs(x3, knots = 6))
> m2 <- gamboost(y ~ (bbs(x1, knots = 6) + bbs(x2, knots = 6)) %X% bbs(x3, knots = 6))
> 
> ### varying numeric
> m1 <- gamboost(y ~ bbs(x1) %X% bols(x2, intercept = FALSE, lambda = 0))
> m2 <- gamboost(y ~ bbs(x1, by = x2, df = 4))
> stopifnot(max(abs(predict(m1) - predict(m2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(m1, newdata = ndf) - predict(m2, newdata = ndf))) < sqrt(.Machine$double.eps))
> 
> ### varying factor
> m1 <- gamboost(y ~ bbs(x1) %X% bols(f, df = 5, contrasts.arg = "contr.dummy"))
Warning message:
In data.frame(..., check.names = FALSE) :
  row names were found from a short variable and have been discarded
> coef(m1)
$`"bbs(x1) %X% bols(f, df = 5, contrasts.arg = \"contr.dummy\")"`
 [1]  0.2242693758  0.0244549055 -0.0018236656 -0.0649204417 -0.0158616261
 [6] -0.1891022703  0.0633957967 -0.1288712394 -0.3211539316 -0.4271143516
[11]  0.1304389247 -0.1279015955 -0.2844975667 -0.5354756406  0.1377335871
[16]  0.0453968886 -0.0338951732 -0.1953318221 -0.0365952954  0.2189850221
[21]  0.3040001645  0.4485289400 -0.0716853706  0.0631618066  0.3325198072
[26]  0.3788911610  0.2873215520 -0.1578934092  0.0374947404 -0.1538460700
[31]  0.0321145668 -0.1042998008 -0.1475466005 -0.3312423509 -0.3729888541
[36] -0.1410106097 -0.1263566256 -0.0751116331 -0.0884109521 -0.2399923387
[41] -0.0389393947  0.1848036883  0.1656488243  0.0090057253  0.0241351838
[46]  0.0880914017  0.1332083246  0.2231047247  0.0008369162 -0.1344615659
[51]  0.0073865257  0.0724598930  0.0446383409 -0.1022702643  0.0035663694
[56]  0.0550943778 -0.0316890953  0.2015664246 -0.0822295181 -0.0247739018
[61] -0.2539099743  0.0695884531  0.0758519443 -0.4923737237 -0.0674229150
[66]  0.0138897271  0.1275144137 -0.1310550250 -0.0040526037  0.3899574508
[71]  0.3122233938  0.1306996445 -0.0656754995  0.6050839245  0.3881688503
[76]  0.2096929714 -0.0502131903  0.2857074658  0.3854780456  0.1159202913
[81] -0.1884282320 -0.2241611822  0.5098269614 -0.0069895193 -0.1851369840
[86] -0.6265554226  0.4419209852 -0.0189754388 -0.0917695408 -0.5729191166
[91]  0.2786709776 -0.0124807032  0.0007989512 -0.2654733304  0.0868918239
[96] -0.0029929838

attr(,"offset")
[1] 0.03826691
> predict(m1, newdata = ndf)
             [,1]
 [1,]  0.02564341
 [2,]  0.01259759
 [3,]  0.05543313
 [4,] -0.12988977
 [5,] -0.03716691
 [6,] -0.02293550
 [7,] -0.01661927
 [8,] -0.09304843
 [9,]  0.06323357
[10,] -0.13853039
> 
> ### cbind
> m1 <- gamboost(y ~ bols(x1, intercept = FALSE, df = 1) %+%
+                    bols(x2, intercept = FALSE, df = 1))
> m2 <- gamboost(y ~ bols(x1, x2, intercept = FALSE, df = 2))
> stopifnot(max(abs(predict(m1) - predict(m2))) < sqrt(.Machine$double.eps))
> stopifnot(max(abs(predict(m1, newdata = ndf) - predict(m2, newdata = ndf))) < sqrt(.Machine$double.eps))
> 
> ### yeah
> m1 <- gamboost(y ~ (bols(x1, intercept = FALSE, df = 1) %+%
+                     bols(x2, intercept = FALSE, df = 1)) %X% bols(f, df = 4) +
+                     bbs(x1) + bspatial(x1, x2))
Warning message:
In data.frame(..., check.names = FALSE) :
  row names were found from a short variable and have been discarded
> 
> 
> ### test bmono with categorical covariates
> set.seed(781)
> x <- as.ordered(gl(4, 50))
> y <- rnorm(200, mean = rep(c(1,3,2,4), each = 50), sd = 1)
> mod1 <- gamboost(y ~ bols(x))
> diff(coef(mod1)[[1]])
       x2        x3        x4 
 3.773862 -1.313469  1.937386 
> mod2 <- gamboost(y ~ bmono(x, lambda2 = 10^15))
> stopifnot(abs(coef(mod1)[[1]] - coef(mod2)[[1]])[c(1,4)] < 1e-5)
> stopifnot(all(diff(coef(mod2)[[1]]) > - sqrt(.Machine$double.eps)))
> 
> ### test bmono for tensor-product splines
> x1 <- runif(100, min = -2, max = 3)
> x2 <- runif(100, min = -2, max = 3)
> f <- function(x1, x2)
+     0.5 + x1^2 * (x2 + 3)
> y <- rnorm(100, mean = f(x1, x2), sd = 0.5)
> mod1 <- mboost(y ~ bspatial(x1, x2, df = 6, knots = list(x1 = 10, x2 = 5)),
+                control = boost_control(mstop = 50))
> mod21 <- mboost(y ~ bmono(x1, x2, df = 6, knots = list(x1 = 10, x2 = 5),
+                           lambda2 = list(x1 = 10e6, x2 = 0)),
+                 control = boost_control(mstop = 50))
There were 50 or more warnings (use warnings() to see the first 50)
> mod22 <- mboost(y ~ bmono(x1, x2, df = 6, knots = list(x1 = 10, x2 = 5),
+                           lambda2 = list(x1 = 0, x2 = 10e6)),
+                 control = boost_control(mstop = 50))
There were 50 or more warnings (use warnings() to see the first 50)
> 
> diff_order <- 1
> D1 <- kronecker(diff(diag(10 + 3 + 1), differences = diff_order),
+                 diag(5 + 3 + 1))
> D2 <- kronecker(diag(10 + 3 + 1), diff(diag(5 + 3 + 1),
+                               differences = diff_order))
> 
> beta <- coef(mod1)[[1]]
> sum((D1 %*% beta)[D1 %*% beta <= 0])
[1] -117.1438
> sum((D2 %*% beta)[D2 %*% beta <= 0])
[1] 0
> 
> beta <- coef(mod21)[[1]]
> sum((D1 %*% beta)[D1 %*% beta <= 0])
[1] -0.0001055744
> sum((D2 %*% beta)[D2 %*% beta <= 0])
[1] -0.003965627
> stopifnot(all(D1 %*% beta > - 2e-05))
> 
> beta <- coef(mod22)[[1]]
> sum((D1 %*% beta)[D1 %*% beta <= 0])
[1] -117.4581
> sum((D2 %*% beta)[D2 %*% beta <= 0])
[1] 0
> stopifnot(all(D2 %*% beta > - 2e-05))
> 
> #nd <- expand.grid(sort(x1), sort(x2))
> #names(nd) <- c("x1", "x2")
> #contour(sort(x1), sort(x2),
> #        z = matrix(predict(mod1, newdata = nd), ncol = length(x1)))
> #contour(sort(x1), sort(x2),
> #        z = matrix(predict(mod21, newdata = nd), ncol = length(x1)),
> #        col = "red", add = TRUE)
> #contour(sort(x1), sort(x2),
> #        z = matrix(predict(mod22, newdata = nd), ncol = length(x1)),
> #        col = "green", add = TRUE)
> 
> ### %O%: kronecker product of baselearners for matrix-valued
> ### responses
> x1 <- 1:10/10
> x2 <- 11:17/17
> 
> X1 <- cbind(1, x1, x1^2)
> colnames(X1) <- paste("X1", 1:ncol(X1), sep = "_")
> X2 <- cbind(1, x2, x2^2, x2^3)
> colnames(X2) <- paste("X2", 1:ncol(X2), sep = "_")
> 
> x <- expand.grid(x1, x2)
> colnames(x) <- c("x1", "x2")
> B1 <- with(x, cbind(1, x1, x1^2))
> colnames(B1) <- paste("B1", 1:ncol(B1), sep = "_")
> B2 <- with(x, cbind(1, x2, x2^2, x2^3))
> colnames(B2) <- paste("B2", 1:ncol(B2), sep = "_")
> 
> X <- kronecker(X2, X1)
> w <- rep(1, nrow(X))
> 
> B <- kronecker(matrix(1, ncol = ncol(B2)), B1) *
+      kronecker(B2, matrix(1, ncol = ncol(B1)))
> 
> tol <- 1 / 10000
> stopifnot(max(abs(X - B)) < tol)
> 
> K1 <- diag(ncol(B1))
> K2 <- crossprod(diff(diag(ncol(B2)), diff = 2))
> 
> b1 <- buser(B2, K = diag(ncol(B2))) %X%
+             buser(B1, K = diag(ncol(B1)))
> 
> stopifnot(max(abs(extract(b1, "design") - B)) < tol)
> b1d <- b1$dpp(w)
> 
> b2 <- buser(X1, K = diag(ncol(X1))) %O%
+       buser(X2, K = diag(ncol(X2)))
> b2d <- b2$dpp(w)
> 
> stopifnot(max(abs(get("XtX", environment(b1d$fit)) -
+         get("XtX", environment(b2d$fit)))) < tol)
> 
> y <- runif(nrow(X))
> 
> stopifnot(max(abs(b1d$fit(y)$model - as.vector(b2d$fit(y)$model))) < 1/1000)
> 
> m1 <- b1d$fit(y)
> p1 <- b1d$predict(list(m1, m1))
> 
> m2 <- b2d$fit(y)
> p2 <- b2d$predict(list(m2, m2))
> 
> stopifnot(max(abs(p1 - p2)) < tol)
> 
> 
> x1 <- runif(200)
> x2 <- runif(60)
> y <- rnorm(length(x1) * length(x2))
> d <- expand.grid(x1, x2)
> d$y <- y
> w <- as.vector(rmultinom(1, length(y), rep(1 / length(y), length(y))))
> 
> m1 <- mboost(y ~ bbs(Var2, df = 3, knots = 5) %X%
+                      bbs(Var1, df = 3, knots = 7),
+                      data = d, weights = w)
> m2 <- mboost(y ~ bbs(x1, df = 3, knots = 7)%O%
+                  bbs(x2, df = 3, knots = 5),
+                  weights = w)
> 
> stopifnot(max(abs(fitted(m1) - fitted(m2))) < tol)
> 
> stopifnot(max(abs(coef(m1)[[1]] - coef(m2)[[1]])) < tol)
> 
> stopifnot(max(abs(m1$predict() - m2$predict())) < tol)
> 
> p1 <- predict(m1, newdata = expand.grid(Var1 = c(0.2, 0.5), Var2 = c(0.7, 0.3)))
> p2 <- predict(m2, newdata = data.frame(x1 = c(0.2, 0.5), x2 = c(0.7, 0.3)))
> 
> stopifnot(max(abs(p1 - p2)) < tol)
> 
> 
> proc.time()
   user  system elapsed 
 25.041   0.424  25.576 
